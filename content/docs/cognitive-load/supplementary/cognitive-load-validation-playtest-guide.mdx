---
title: cognitive-load-validation-playtest-guide
---
# Cognitive Load Validation Playtest Guide

## Testing and Validating Cognitive Load Design Decisions

**Purpose:** This guide provides concrete methods for playtesting and validating cognitive load choices. Use it to test whether your game's mental demands match your aesthetic goals and audience capacity.

**When to use:**

* After initial system design (to validate complexity is appropriate)
* When players say "it's too complicated" or "it's confusing"
* Before finalizing UI and information architecture
* After adding new systems or features
* When targeting different audience segments (casual vs. hardcore)

**Integration:** This guide complements Chapter 4 (Cognitive Load) design framework and feeds into iteration and polish phases.

***

## Part 1: Pre-Playtest Setup

### 1.1 Define Load Budget and Allocation

Before playtesting, explicitly state your cognitive load targets.

**Template:**

```
Target Audience: [Casual/Midcore/Hardcore]
Investment Profile: [Time per session, cognitive capacity, experience level]
Total Load Budget: [3-5/5-8/8-15+ units - qualitative estimate]

Load Allocation (by source):
- Core Mechanic: [%/description]
- Resource Management: [%/description]
- Goal Tracking: [%/description]
- Context/Meta-Progression: [%/description]
- Social (if multiplayer): [%/description]

Success Looks Like:
- [Observable behaviors indicating appropriate load]
- [Player statements indicating engagement not overwhelm]

Failure Looks Like:
- [Observable behaviors indicating overload]
- [Player statements indicating confusion or frustration]
```

**Example - Midcore roguelike:**

```
Target Audience: Midcore
Investment Profile: 30-60 min sessions, moderate gaming experience, wants depth but has time constraints
Total Load Budget: 6-7 units (medium-high)

Load Allocation:
- Core Mechanic (combat + build decisions): 40% (execution + decision load)
- Resource Management (HP, gold, items): 20% (tracking + decision load)
- Goal Tracking (run objectives, meta unlocks): 15% (tracking load)
- Context/Meta-Progression (permanent upgrades, character relationships): 25% (tracking + decision load)

Success Looks Like:
- Players complete 3-5 runs per session without fatigue
- Players make build decisions in <2 minutes
- Players can resume after 1 week without confusion
- Players say "one more run" (flow state achieved)

Failure Looks Like:
- Players quit after 1 run due to exhaustion
- Analysis paralysis on build choices (>5 min deliberation)
- Can't remember what they were doing after 2 days away
- Players say "too much to keep track of"
```

**Exercise:** Write complete load profile for your game before playtesting.

### 1.2 Identify Load Type Priorities

For your archetype and aesthetic, which load types are primary vs. secondary?

**Template:**

```
Archetype: [From Chapter 2]
Primary Load Type: [Tracking/Decision/Execution]
Secondary Load Type: [Tracking/Decision/Execution]
Minimal Load Type: [Tracking/Decision/Execution]

Rationale: [Why this profile serves your aesthetic]
```

**Example - Tactical combat game:**

```
Archetype: Tactical Combat (turn-based)
Primary Load Type: Decision (evaluating positioning, target priority, ability usage)
Secondary Load Type: Tracking (unit positions, abilities, cooldowns)
Minimal Load Type: Execution (turn-based, no timing requirements)

Rationale: Turn-based structure allows high decision load. Challenge aesthetic requires meaningful tactical choices. Low execution load makes it accessible to players without fast reflexes.
```

**Example - Real-time action game:**

```
Archetype: Real-Time Action
Primary Load Type: Execution (timing, aiming, dodging)
Secondary Load Type: Tracking (health, ammo, enemy positions - must be instantly readable)
Minimal Load Type: Decision (priorities must be obvious under time pressure)

Rationale: Real-time pressure demands fast execution. Challenge aesthetic through mechanical skill. Low decision load prevents analysis paralysis during combat.
```

**Exercise:** Map your game to load type priorities. If your allocation doesn't match archetype's natural profile (from Chapter 4 matrix), document why and prepare to test carefully.

### 1.3 Define Key Validation Questions

**By Load Type:**

**Tracking Load:**

* "Can players remember important state without external notes?"
* "How often do players check UI to verify information?"
* "Can players resume play after interruption without confusion?"
* "Do players forget critical information during play?"

**Decision Load:**

* "How long do strategic decisions take?"
* "Do players exhibit analysis paralysis?"
* "Are players confident in their choices?"
* "Do players use external tools (calculators, guides) for basic decisions?"

**Execution Load:**

* "Do players successfully execute intended actions?"
* "Does execution improve with practice?"
* "Are failures due to lack of knowledge or lack of execution skill?"
* "Do players express frustration at mechanical difficulty?"

**Total Load:**

* "How long can players play before mental fatigue?"
* "Do players take breaks during sessions? How often?"
* "Do players return for additional sessions?"
* "Do players recommend the game to others in target audience?"

**Exercise:** Write 5-7 validation questions specific to your game's load profile.

### 1.4 Prepare Observation Framework

**What to track during playtests:**

**Behavioral Observations:**

* UI checking frequency (tracking load indicator)
* Pause frequency and duration (processing time indicator)
* Note-taking or external tool usage (offloading indicator)
* Mistakes and errors (load exceeding capacity)
* Hesitation before actions (decision load indicator)

**Temporal Observations:**

* Time to complete tutorial/onboarding
* Time to achieve flow state (if at all)
* Session duration before fatigue
* Time between sessions (return rate)
* Time to resume play after break

**Verbal Observations:**

* Questions asked ("what does this do?")
* Confusion statements ("I don't understand")
* Overwhelm statements ("too much going on")
* Satisfaction statements ("this is fun")
* Strategy discussions (decision-making process)

**Physical Observations:**

* Leaning forward (engagement) vs. leaning back (disengagement)
* Facial expressions (frustration, confusion, satisfaction)
* Speed of actions (confident vs. hesitant)
* Multi-tasking attempts (checking phone = disengagement)

**Observation Template:**

```
PLAYTEST SESSION: [Date/Session Number]
LOAD FOCUS: [Tracking/Decision/Execution/Total]
PLAYTESTER PROFILE: [Casual/Midcore/Hardcore, gaming experience]

COGNITIVE LOAD INDICATORS:

UI Checks (Tracking Load):
- [Timestamp] Checked [element] - [Frequency: Once/Repeatedly/Constantly]

Pauses (Processing Time):
- [Timestamp] Paused for [duration] - [Context: decision point/confusion/overwhelm]

Decision Time:
- [Timestamp] [Decision type] took [duration] - [Confident/Hesitant/Paralyzed]

Execution Errors:
- [Timestamp] Failed to execute [action] - [Knowledge gap/Skill gap/UI unclear]

Verbal Indicators:
- [Timestamp] "[Direct quote]" - [Context]

Emotional States:
- [Timestamp] [Engaged/Frustrated/Confused/Flow state] - [Trigger]

Session Metrics:
- Total duration: [minutes]
- Breaks taken: [number, duration]
- Tutorial completion: [yes/no, time]
- Flow state achieved: [yes/no, when]
- Wanted to continue: [yes/no]

RED FLAGS:
- [Clear overload indicators]

POSITIVE SIGNS:
- [Load appropriate indicators]
```

***

## Part 2: Load Type Specific Validation Tests

### 2.1 Tracking Load Validation

**Goal:** Verify players can monitor and remember necessary state without overwhelming.

**Test 1: The UI Check Frequency Test**

**Method:**

1. Video record playtest session
2. Count how often player checks each UI element
3. Calculate checks per minute for critical information

**Analysis:**

```
<1 check per minute → Information internalized (good) OR information irrelevant (bad)
1-3 checks per minute → Healthy monitoring (appropriate tracking load)
3-6 checks per minute → High attention demand (verify this serves aesthetic)
>6 checks per minute → Overwhelming, cannot internalize (too much tracking load)
```

**Example - Health bar checking:**

```
Session 1: Player checks health 15 times in 10 minutes = 1.5 checks/min
Analysis: Healthy monitoring. Player aware of health state, checking strategically.
APPROPRIATE TRACKING LOAD.

Session 2: Player checks health 80 times in 10 minutes = 8 checks/min
Analysis: Cannot internalize health state, constantly verifying. 
Either UI unclear OR health changing too fast OR player overwhelmed.
TRACKING LOAD TOO HIGH or UI PROBLEM.
```

**Test 2: The Interruption Recovery Test**

**Method:**

1. Interrupt player mid-session (fake phone call, urgent message)
2. Return after 30 seconds
3. Observe how long until player resumes confident play
4. Ask "What were you doing? What's your current situation?"

**Analysis:**

```
<10 seconds to resume → Good state visibility, low tracking load
10-30 seconds to resume → Some re-orientation needed, acceptable
30-60 seconds to resume → High tracking load, lots to remember
>60 seconds or can't resume → Tracking load exceeds capacity
```

**Example - Tactical combat game:**

```
Player interrupted during enemy turn.
Returns 30 seconds later.
Immediately identifies: "I have 3 units, enemies are flanking left, I need to defend that position."
Resumes play in 8 seconds.
GOOD: State is visible and internalized. Tracking load appropriate.

Alternative scenario:
Returns 30 seconds later.
"Uh... what was I doing? Which units are mine? What are their abilities? Where was I going?"
Takes 90 seconds scrolling through UI, checking unit cards, verifying state.
BAD: Too much state to reconstruct. Tracking load too high or UI support insufficient.
```

**Test 3: The Session Gap Memory Test**

**Method:**

1. Players play session 1
2. Return 3-7 days later for session 2
3. Before resuming: Ask "What were you doing last time?"
4. Observe how long until they re-engage

**Analysis:**

```
Casual games: Should remember in <2 minutes with no external aids
Midcore games: Should remember in <5 minutes, journal/recap helpful
Hardcore games: May take 5-10 minutes, complex state acceptable

Can't remember core goal → Context/goal tracking too complex
Can't remember mechanics → Too many mechanics OR poor tutorial
Can't remember meta-progression → Meta-systems too complex
```

**Example - RPG campaign:**

```
Week gap between sessions.
Player returns: "I was trying to reach the northern ruins to find the artifact."
Checks journal briefly (30 sec) to remember NPC names and active quests.
Resumes play in 3 minutes.
APPROPRIATE: Macro goal remembered, details externalized to journal.

Alternative scenario:
Week gap between sessions.
Player returns: "Uh... I think I was doing something? There was a dungeon? Or a town?"
Spends 10 minutes reading quest log trying to understand situation.
Still unclear what the goal is.
BAD: Too many simultaneous quests OR goals unclear OR too complex narrative.
```

**Test 4: The Multi-Resource Tracking Test**

**Method:**

1. Count distinct resources/trackables in your game
2. During play, randomly pause and ask player to state current values WITHOUT LOOKING AT UI
3. Record accuracy

**Analysis:**

```
3 or fewer resources: Players should recall 100% accurately
4-5 resources: Players should recall 80%+ accurately
6-7 resources: Players recall 60-70% (acceptable for midcore)
8+ resources: Players recall <50% (only acceptable for hardcore with tools)
```

**Example - Survival game:**

```
Game has 6 resources: Health, Hunger, Thirst, Temperature, Stamina, Sanity

Pause mid-play: "Tell me your current resource levels without looking."
Player responds:
- "Health is about 70%... or maybe 60%?"
- "Hunger is getting low, like 30%"
- "Thirst... uh... not sure, I think I drank recently?"
- "Temperature is cold, I saw the icon"
- "Stamina is fine"
- "Sanity? Oh yeah, that exists. No idea."

Accuracy: 3/6 resources known, 2 approximate, 1 forgotten
Analysis: 6 resources exceeding casual/midcore tracking capacity.
Action: Reduce to 4 core resources OR chunk (combine Hunger/Thirst into "Sustenance")
```

**Test 5: The External Notes Test**

**Method:**

1. Provide paper and pen
2. Observe if/when players take notes
3. Review notes to see what players felt they needed to externalize

**Analysis:**

```
No notes taken → Either load appropriate OR player not engaged enough to bother
Notes on goals/quests → Acceptable for complex narratives
Notes on mechanics/numbers → Mechanics too complex, poor UI reference
Notes on strategies → Good sign (player thinking deeply)
Notes on basic info (controls, resources) → Tutorial failure or UI failure
```

**Example - Investigation game:**

```
Players create evidence boards with connections between clues.
This is INTENDED - deduction is the aesthetic.
External notes appropriate for Investigation archetype.

Alternative game - Action shooter:
Players writing down weapon stats, ability cooldowns, enemy weaknesses.
This is NOT intended - information should be learnable through play.
Too much complexity for Action archetype.
```

### 2.2 Decision Load Validation

**Goal:** Verify decision complexity matches aesthetic intent and doesn't cause paralysis.

**Test 6: The Decision Time Distribution Test**

**Method:**

1. Identify key decision points in your game
2. Time how long each decision takes
3. Categorize: Instant (\<2 sec), Tactical (2-30 sec), Strategic (30 sec - 5 min), Analysis Paralysis (>5 min)

**Analysis:**

```
Casual games: Mostly Instant, some Tactical
Midcore games: Mostly Tactical, some Strategic
Hardcore games: Mix of all, Strategic acceptable

If 50%+ decisions cause Analysis Paralysis → Decision load too high
If 90%+ decisions are Instant → May lack strategic depth (if depth intended)
```

**Example - Deck-building game:**

```
20 card draft choices tracked:
- 14 decisions: 5-15 seconds (Tactical) - "Does this fit my deck?"
- 5 decisions: 30 sec - 2 min (Strategic) - "Does this change my strategy?"
- 1 decision: 6 minutes (Analysis Paralysis) - Complex synergy evaluation

Analysis: Mostly healthy decision load. 1 AP instance acceptable if rare.
If all decisions took 6 minutes → Too complex.
If all decisions took 5 seconds → Too simple.
BALANCED for midcore audience.
```

**Test 7: The Build Diversity Test**

**Method:**

1. Observe 5+ players making character/build decisions
2. Record what they choose
3. Check for variety vs. convergence

**Analysis:**

```
All players choose same build → Either dominant strategy (balance problem) OR other options too complex to evaluate (decision load too high)
Wide variety → Healthy decision space, load appropriate
Players always choose "recommended" builds → Not engaging with decision, either too complex or they don't care
```

**Example - RPG skill trees:**

```
5 players, skill point allocation:
Player 1: Focused fire magic build
Player 2: Hybrid fire/ice build
Player 3: Full defense tank build
Player 4: Fire magic (copied from guide)
Player 5: Fire magic (said it "seemed easiest")

Analysis: 3 diverse builds (good), 2 fire defaults (concerning).
Interview Players 4-5: "Other options looked complicated" "Fire was obviously best for damage"
Problem: Other builds not clearly viable OR evaluation too complex.
Action: Clarify other build strengths, simplify comparisons.
```

**Test 8: The Regret Quality Test**

**Method:**

1. After session, ask "What decision do you regret?"
2. Follow up: "Why? What would you do differently?"
3. Assess if regret comes from learning (good) or confusion (bad)

**Analysis:**

```
"I should have saved my gold for better item" → Learning-based regret (good)
"I didn't know that ability existed" → Information problem (bad)
"I couldn't tell which option was better" → Unclear values (decision load too high)
"I didn't understand what the choice meant" → Comprehension problem (bad)
No regrets + no engagement → Either too easy or player not invested
```

**Example:**

```
Player: "I regret choosing the fire sword over the ice axe."
Why: "I didn't realize the boss was fire-immune. I should have checked."
Analysis: GOOD regret - player learning game systems, had information available.

Alternative:
Player: "I regret choosing the fire sword over the ice axe."
Why: "I don't understand how weapon damage works. There were too many numbers."
Analysis: BAD regret - comprehension problem, decision too complex.
Action: Simplify weapon comparison OR improve UI presentation.
```

**Test 9: The Optimal Play Calculation Test**

**Method:**

1. Present a decision scenario (build choice, tactical situation, resource allocation)
2. Ask player: "What's the best choice here and why?"
3. Observe if they can evaluate or if they use external tools/guides

**Analysis:**

```
Can explain reasoning → Decision load manageable, values clear
Uses mental estimation → Healthy strategic thinking
Uses calculator for basic decisions → Math too complex
Immediately checks guide → Either optimization culture OR can't evaluate independently
Says "I don't know, I just pick one" → Values unclear OR too many variables
```

**Example - Tactical combat:**

```
Scenario: "Should you move unit A forward or keep it back?"
Player: "If I move forward I can flank but I'll be exposed. I'll stay back and shoot."
Reasoning clear, evaluated trade-offs.
DECISION LOAD APPROPRIATE.

Alternative:
Player: "Uh... which one is better?" [Opens damage calculator]
[Spends 3 minutes calculating DPS, accuracy, cover bonuses]
"I guess... move forward? Maybe?"
DECISION LOAD TOO HIGH - too many variables, unclear evaluation.
```

### 2.3 Execution Load Validation

**Goal:** Verify execution demands match player capacity and aesthetic goals.

**Test 10: The Learning Curve Test**

**Method:**

1. Track success rate of key actions across first 5 sessions
2. Plot improvement trajectory
3. Compare to target mastery curve

**Analysis:**

```
Success rate Session 1 → Session 5:
Expected casual game: 70% → 90% (quick mastery)
Expected midcore game: 50% → 80% (moderate learning)
Expected hardcore game: 30% → 70% (long mastery curve)

If no improvement → Execution beyond player capacity
If immediate mastery → May lack execution challenge (if challenge intended)
```

**Example - Platformer:**

```
Jump timing success rate tracked:
Session 1: 40% → Session 2: 55% → Session 3: 70% → Session 4: 80% → Session 5: 85%

Analysis: Clear learning curve, player improving with practice.
Execution load appropriate for Challenge aesthetic.
If stuck at 40% → Execution too difficult, widen timing window.
If immediate 90% → Too easy, tighten timing for more challenge.
```

**Test 11: The Knowledge vs. Skill Gap Test**

**Method:**

1. When player fails action, ask: "Did you know what to do?"
2. If yes → Execution problem
3. If no → Knowledge problem

**Analysis:**

```
Mostly knowledge problems → Tutorial/communication issue, not execution load
Mostly execution problems → Execution load appropriate (for Challenge aesthetic) OR too high (for other aesthetics)
Mixed → Normal learning process
```

**Example - Fighting game combo:**

```
Player attempts combo, fails 8/10 times.
"Did you know what inputs to do?" "Yes, I know the sequence."
"Why did it fail?" "I can't get the timing right."
→ EXECUTION problem. Appropriate for fighting game Challenge aesthetic.

Alternative:
Player attempts combo, fails 8/10 times.
"Did you know what inputs to do?" "Not really, I'm not sure what buttons to press."
→ KNOWLEDGE problem. Tutorial failure, not execution load issue.
```

**Test 12: The Execution Stress Test**

**Method:**

1. Observe execution performance in low-pressure vs. high-pressure scenarios
2. Check if execution degrades under stress

**Analysis:**

```
Performance degrades slightly under pressure → Normal, acceptable
Performance collapses under pressure → Execution load too high, no bandwidth for stress
Performance unchanged → Either execution automated OR no meaningful pressure
```

**Example - Shooter game:**

```
Target practice (no enemies): 80% accuracy
Combat (enemies shooting back): 50% accuracy
Analysis: Pressure affects performance but still functional.
Execution load + stress load = appropriate combined load.

Alternative:
Target practice: 80% accuracy
Combat: 10% accuracy (player panics, can't aim at all)
Analysis: Execution load + stress load exceeds capacity.
Action: Reduce execution demands OR reduce pressure sources.
```

**Test 13: The Accessibility Barrier Test**

**Method:**

1. Playtest with diverse player capabilities (age, experience, physical ability)
2. Identify if execution load creates exclusions

**Analysis:**

```
If game targets Challenge aesthetic with execution focus → High execution load acceptable (targeting skilled players)
If game targets other aesthetics → Execution barriers may exclude intended audience
Consider accessibility options: Aim assist, timing windows, input simplification
```

**Example - Narrative game:**

```
Aesthetic: Narrative (story experience)
Execution: QTE sequences requiring fast button presses

Player: "I want to see the story but I keep failing these action sections."
Analysis: Execution load contradicts aesthetic goal.
Action: Add "story mode" with automatic QTE success OR remove QTEs entirely.
```

### 2.4 Total Load Validation

**Goal:** Verify combined load from all sources doesn't exceed player capacity.

**Test 14: The Session Duration Test**

**Method:**

1. Observe natural session length before player stops
2. Note reason for stopping: Satisfied completion, mental fatigue, external interruption, boredom

**Analysis:**

```
Casual target: 15-30 min → stops satisfied
Midcore target: 30-90 min → stops satisfied or external interruption
Hardcore target: 60-240+ min → stops satisfied or external interruption

If stopping due to mental fatigue before aesthetic payoff → Total load too high
If stopping due to boredom before target duration → Load too low OR content insufficient
```

**Example - Roguelike (30 min target run duration):**

```
Session 1: Player plays 1 run (28 min), stops satisfied
Session 2: Player plays 3 runs (85 min total), says "one more run", external interruption
Session 3: Player plays 2 runs (55 min), stops satisfied

Analysis: Players achieving flow state, playing multiple runs, stopping satisfied.
Total load appropriate for engagement without fatigue.

Alternative:
Session 1: Player plays 1 run (28 min), says "that was exhausting"
Session 2: Player plays 1 run (32 min), stops saying "I need a break"
Session 3: Player skips session entirely

Analysis: Total load too high - single run causing fatigue.
Action: Reduce complexity, simplify systems, improve UI.
```

**Test 15: The "One More" Test**

**Method:**

1. At natural stopping point, observe if player continues
2. Note if continuation is eager ("one more!") or obligatory ("I should finish this")

**Analysis:**

```
Eager continuation → Flow state achieved, load appropriate
Reluctant continuation → External motivation (completionism) not intrinsic engagement
Stops at stopping point → Either satisfied OR fatigued (interview to distinguish)
```

**Example:**

```
Player completes dungeon run.
Immediately: "Okay, one more run before bed."
Continues for 2 more runs, finally stops at 11pm saying "I should sleep."

Analysis: Flow state, intrinsic motivation, load appropriate.

Alternative:
Player completes dungeon run.
"Okay, I should do another to unlock that item."
Plays second run looking tired, stops immediately after.
"That's enough."

Analysis: Extrinsic motivation (completion goal) not flow state.
Load may be fatiguing even if player pushes through.
```

**Test 16: The Multi-Session Return Test**

**Method:**

1. Track if players return for subsequent sessions
2. Note gaps between sessions
3. Interview about why they returned or didn't

**Analysis:**

```
Returns within 1-3 days → Strong engagement
Returns after 1-2 weeks → Moderate engagement
Doesn't return → Either not interested OR overwhelmed (interview to determine)

"I wanted to keep playing" → Good
"I needed a break from the complexity" → Total load too high
"I forgot how to play" → Tutorial/resumption problem
```

**Test 17: The Tutorial Completion Test**

**Method:**

1. Track tutorial/onboarding completion rate
2. Observe where players quit or get stuck
3. Measure time to reach "actual gameplay"

**Analysis:**

```
>80% complete tutorial → Good onboarding
50-80% complete → Some friction, investigate dropout points
<50% complete → Tutorial too long OR too complex OR immediate overload

Time to gameplay:
Casual target: <5 minutes
Midcore target: 5-15 minutes  
Hardcore target: 15-30 minutes acceptable

Exceeding target → Front-loading too much cognitive load
```

**Example:**

```
Tutorial: 10 minutes, teaches 5 mechanics
Completion rate: 85%
15% dropout point: Mechanic 4 introduction (8 min in)

Interview dropouts: "Too many things to remember"
Analysis: Trying to teach too much too fast.
Action: Progressive disclosure - teach only 2-3 mechanics in tutorial, reveal others during play.
```

***

## Part 3: Archetype-Specific Load Validation

### 3.1 Investigation Archetype (High Tracking, Medium Decision)

**Special focus:** Evidence tracking, connection-making, theory-forming

**Additional tests:**

**Test 18: The Clue Recall Test**

* Pause mid-investigation: "What clues have you found?"
* Players should recall 60-80% of important clues
* If \<40% → Too many clues OR clues not distinctive
* If journal required for basics → Tracking load as intended for archetype

**Test 19: The Connection Making Test**

* Present player with 3 clues, ask "What do these tell you?"
* Can they form theory? → Decision load appropriate
* Can't connect dots? → Either clues too obscure OR too much information

**Archetype-appropriate load profile:**

* High tracking of evidence = intended (serves Discovery aesthetic)
* Medium decision making connections = intended
* External notes common = acceptable for archetype
* If players can't track basic case details → Too complex even for Investigation

### 3.2 Tactical Combat Archetype (Medium Tracking, High Decision)

**Special focus:** Positional evaluation, action economy, ability interactions

**Additional tests:**

**Test 20: The Threat Evaluation Test**

* Pause mid-combat: "Which enemy is most dangerous and why?"
* Clear answer? → Threat visibility good, tracking load appropriate
* "I don't know, they all look the same" → Enemy differentiation poor

**Test 21: The Action Economy Test**

* Time how long turn planning takes
* Casual: \<30 sec per turn OR not target audience
* Midcore: 30 sec - 2 min per turn
* Hardcore: 2-5 min per turn acceptable
* > 5 min → Analysis paralysis, decision load too high

**Archetype-appropriate load profile:**

* Medium tracking = intended (positions, abilities, cooldowns)
* High decision = intended (serves Challenge aesthetic)
* If combat feels random not strategic → Decision space unclear
* If players always do same action → False choices OR dominant strategy

### 3.3 Real-Time Action Archetype (Low Tracking, Low Decision, High Execution)

**Special focus:** Execution under time pressure, instant information visibility

**Additional tests:**

**Test 22: The UI Glanceability Test**

* During combat, can player state critical info without pausing?
* Health/resources should be instantly readable (split-second glance)
* If requires reading numbers → UI not optimized for real-time
* Color coding, icons, audio cues better than numbers

**Test 23: The Priority Clarity Test**

* Stop during combat: "What should you be doing right now?"
* Immediate clear answer? → Priorities clear, decision load low
* "Uh... attack? Dodge? Get pickup?" → Priorities unclear, too much to evaluate

**Archetype-appropriate load profile:**

* Low tracking = required (can't monitor complex state during action)
* Low decision = required (can't analyze under time pressure)
* High execution = intended (serves Challenge aesthetic through skill)
* If players overwhelmed by info → Reduce tracking/decision load further
* If players mastering too quickly → Increase execution challenge, NOT complexity

### 3.4 Cozy Simulation Archetype (Low Everything)

**Special focus:** Relaxation, flow state, minimal stress

**Additional tests:**

**Test 24: The Stress Response Test**

* During play, monitor player body language and statements
* Tension indicators (leaning forward, furrowed brow, frustrated sounds) = load too high
* Relaxation indicators (leaning back, smiling, humming) = appropriate
* "This is so chill" = success
* "I'm stressed about \[system]" = failure

**Test 25: The Abundance Test**

* Track if players ever experience scarcity or pressure
* Cozy aesthetic requires abundance, not constraint
* "I don't have enough \[resource]" = design failure
* "I can do whatever I want" = success

**Archetype-appropriate load profile:**

* Low tracking = required (Submission aesthetic contradicts complexity)
* Low decision = required (no optimization pressure)
* Low execution = required (gentle, forgiving)
* ANY system causing stress → Violates archetype, remove or simplify

### 3.5 4X Strategy Archetype (Very High Tracking, Very High Decision)

**Special focus:** Empire management, interconnected systems, long-term planning

**Additional tests:**

**Test 26: The System Mastery Test**

* After 10+ hours, do players understand system interactions?
* "Tech A enables Strategy B which counters Faction C" = mastery developing
* "I still don't understand what most of this does" = too complex OR poor communication

**Test 27: The Optimization Behavior Test**

* Observe if players optimize (good for Mastery) or feel obligated to optimize (bad)
* "I want to find the perfect build" = intrinsic mastery drive (good)
* "I need a guide to not fall behind" = extrinsic pressure (bad)

**Archetype-appropriate load profile:**

* Very high tracking = intended (serves Mastery aesthetic)
* Very high decision = intended
* External tools expected (wikis, calculators, build planners)
* Multi-hour sessions expected
* If casual players trying this archetype → Wrong audience, not design problem
* If hardcore players overwhelmed → TOO complex (rare but possible)

***

## Part 4: Common Load Problems and Diagnostics

### 4.1 Diagnostic Framework: "Players Say It's Too Complicated"

**Step 1: Identify which load type is the problem**

Ask players specifically:

* "Is it hard to remember what's happening?" → TRACKING load
* "Is it hard to decide what to do?" → DECISION load
* "Is it hard to execute what you want?" → EXECUTION load
* "Is it all too much at once?" → TOTAL load

**Step 2: Diagnose root cause**

**If TRACKING load:**

* Count trackables (resources, status effects, units, etc.)
* More than 5-7 for midcore? → Reduce or chunk
* UI cluttered? → Improve information architecture
* State changes not visible? → Improve feedback

**If DECISION load:**

* Count simultaneous choices at decision points
* More than 5-7 options? → Reduce or group
* Values unclear? → Clarify costs/benefits
* Consequences unpredictable? → Improve information

**If EXECUTION load:**

* Timing windows too tight? → Widen windows
* Too many simultaneous inputs? → Simplify controls
* Improvement not happening? → Beyond player capacity

**If TOTAL load:**

* Sum all sources (core mechanic, resources, goals, context)
* Exceeds budget for audience? → Cut features OR change audience
* Front-loaded? → Progressive disclosure
* No recovery periods? → Add downtime/breaks

**Step 3: Apply appropriate strategy from Chapter 4**

* Automate busywork
* Chunk related information
* Progressive disclosure
* Allow offloading
* Simplify ruthlessly

### 4.2 Rapid Diagnostic Checklist

When playtests show load problems, check these systematically:

**Tracking Load Checklist:**

* [ ] Fewer than 5-7 trackables for midcore (3-5 for casual, 7+ okay for hardcore)
* [ ] All critical information visible in UI without digging
* [ ] Resource values use clear visual language (bars, colors, icons)
* [ ] State changes provide immediate feedback
* [ ] Can interrupt and resume within 30 seconds

**Decision Load Checklist:**

* [ ] Fewer than 5-7 options per decision point
* [ ] Costs and benefits clearly communicated
* [ ] Values comparable (can evaluate trade-offs)
* [ ] Decisions appropriate for intended audience (instant for casual, strategic for hardcore)
* [ ] No dominant strategies (all options viable)

**Execution Load Checklist:**

* [ ] Controls intuitive and documented
* [ ] Timing windows appropriate for target audience
* [ ] Practice improves performance (learning curve exists)
* [ ] Execution difficulty matches aesthetic intent
* [ ] Accessibility options available if needed

**Total Load Checklist:**

* [ ] Tutorial completable in \<5 min (casual), \<15 min (midcore), \<30 min (hardcore)
* [ ] Sessions reach target duration without fatigue
* [ ] Players return for additional sessions
* [ ] "One more" phenomenon observed (flow state)
* [ ] Core mechanic consumes most of budget (40-70%)
* [ ] Supporting systems justified (each serves aesthetic)

**If all boxes checked but still problems:**

* Load allocation may be wrong (core mechanic needs more budget)
* Aesthetic-archetype mismatch (trying to force wrong profile)
* Target audience incorrect (game is actually for different investment level)

### 4.3 Investment-Payoff Diagnostic

**The core question:** For each system/feature, does cognitive investment create proportional aesthetic payoff?

**Diagnostic process:**

1. **List every system/feature that creates cognitive load**

2. **For each, evaluate:**
   * What load does it create? (tracking/decision/execution, magnitude)
   * What aesthetic payoff does it create? (Challenge/Discovery/Mastery/etc.)
   * Is the payoff worth the investment?

3. **Categorize:**
   * High investment, high payoff → KEEP (serves aesthetic)
   * Low investment, high payoff → KEEP (efficient design)
   * Low investment, low payoff → NEUTRAL (harmless but cut if needed)
   * High investment, low payoff → **CUT or REDUCE** (pure friction)

**Example audit:**

```
Feature: Crafting system
Load: High tracking (materials, recipes), Medium decision (what to craft)
Payoff: Low (items available through other means, crafting optional)
Investment-to-payoff: HIGH INVESTMENT, LOW PAYOFF
Action: Cut crafting OR make it central to progression

Feature: Core combat
Load: Medium execution (aiming), Low tracking (health, ammo)
Payoff: High (Challenge aesthetic, this is the game)
Investment-to-payoff: MEDIUM INVESTMENT, HIGH PAYOFF
Action: Keep, possibly increase investment (more depth)

Feature: Quest journal
Load: Low tracking (passive, check when needed)
Payoff: High (goal clarity, enables engagement)
Investment-to-payoff: LOW INVESTMENT, HIGH PAYOFF
Action: Keep, excellent design
```

### 4.4 Audience Mismatch Diagnostic

**Symptom:** Game feels "too hard" or "too simple" across entire test group

**Diagnostic questions:**

1. **Who are your playtesters?**
   * Gaming experience level
   * Time availability
   * Age range
   * Similar game experience

2. **Who is your target audience?**
   * Intended investment profile (casual/midcore/hardcore)
   * Expected time commitment
   * Assumed prior game knowledge

3. **Is there a mismatch?**
   * Testing casual game with hardcore players → Will seem too simple
   * Testing hardcore game with casual players → Will seem too complex
   * Testing midcore game with mixed group → Will get contradictory feedback

**Resolution:**

* If mismatch: Recruit appropriate testers
* If matched: Actual load problem, not audience problem
* If unsure: Test with diverse audience, look for CLEAR audience split

**Example:**

```
Game: Tactical RPG (intended midcore)
Feedback:
- Hardcore testers: "Too easy, no challenge, beat in 5 hours"
- Casual testers: "Too complicated, gave up after tutorial"
- Midcore testers: "Really fun, challenging but fair, 20 hours of play"

Analysis: Design is correct, hitting intended audience.
NOT a load problem - different audiences have different needs.
Action: Market to midcore audience, don't try to please everyone.
```

***

## Part 5: Progressive Disclosure Validation

**Goal:** Verify complexity reveals at appropriate pace for learning curve.

### 5.1 The Complexity Timeline Test

**Method:**

1. Map when each system/mechanic is introduced
2. Track player mastery of each system
3. Verify new systems only introduced after previous mastered

**Template:**

```
Hour 0-1: [Systems introduced]
  - Mastery achieved: [Yes/No, time to master]
Hour 1-3: [New systems introduced]
  - Previous systems mastered? [Yes/No]
  - New systems overwhelming? [Yes/No]
Hour 3-10: [New systems introduced]
  - Load accumulation acceptable? [Yes/No]
```

**Analysis:**

```
If players master system before next introduction → Good pacing
If new system introduced before previous mastered → Too fast, overwhelm risk
If long gaps with no new systems → May be boring (or intentional for Submission)
```

**Example - RPG:**

```
Hour 0-1: Basic combat (attack, defend)
  - Mastery: Yes, by hour 1
Hour 1-3: Magic system introduced
  - Previous mastered: Yes
  - Overwhelming: No, players exploring spells comfortably
Hour 3-5: Crafting introduced
  - Previous mastered: Partial (still learning spell combos)
  - Overwhelming: YES - players saying "too much at once"

Analysis: Introduced crafting too early.
Action: Delay crafting to Hour 6-8, after magic mastered.
```

### 5.2 The Optional Complexity Test

**Method:**

1. Observe which systems players engage with vs. ignore
2. Check if ignored systems are optional or required

**Analysis:**

```
Players ignore optional depth → Fine (serves hardcore who want it)
Players ignore required system → Either too complex OR communicated poorly
Players engage with everything → Good curiosity OR systems are engaging
```

**Example - Skill tree:**

```
Skill tree has 4 branches: Combat, Magic, Stealth, Support

Observations:
- All players max Combat (required for progress)
- 60% of players explore Magic (optional but useful)
- 20% of players explore Stealth (optional, niche)
- 5% of players touch Support (optional, unclear value)

Analysis: 
- Combat: Working (required, engaged)
- Magic: Working (optional depth for interested players)
- Stealth: Working (niche appeal)
- Support: NOT working (too obscure, low engagement)

Action: Either make Support clearer OR accept niche status OR cut it.
```

***

## Part 6: Interface and Information Architecture Validation

**Goal:** Verify UI reduces cognitive load rather than adding to it.

### 6.1 The Information Hierarchy Test

**Method:**

1. Screenshot your UI
2. Ask testers: "What's the most important information on screen?"
3. Check if their answer matches your design intent

**Analysis:**

```
Players identify correct priority → Good hierarchy
Players focus on wrong elements → Visual hierarchy poor
Players say "I don't know, it's all the same" → No hierarchy, overwhelming
```

**Example - Action game HUD:**

```
Designer intent: Health is most important
Tester response: "Uh... the score? Or maybe the minimap?"
Actually health is small number in corner.

Problem: Visual hierarchy doesn't match importance.
Action: Make health large, central, color-coded. Reduce other element sizes.
```

### 6.2 The Glance Time Test

**Method:**

1. During play, pause and ask: "What's your current health?" (or other resource)
2. Time how long it takes to find and read information

**Analysis:**

```
<1 second → Excellent visibility
1-2 seconds → Acceptable
2-5 seconds → Poor visibility, increase prominence
>5 seconds → Critical failure, information effectively hidden
```

**Example:**

```
"What's your mana?"
Player scans screen for 8 seconds, finally finds small blue bar in corner.
"Oh, it's at 40%... I think?"

Problem: Mana visibility too low for resource that affects every decision.
Action: Larger mana bar, better contrast, possibly add numeric display.
```

### 6.3 The Icon Comprehension Test

**Method:**

1. Show UI icons in isolation
2. Ask "What does this mean?"
3. Check comprehension rate

**Analysis:**

```
>80% correct → Icon clear
50-80% correct → Ambiguous, could improve
<50% correct → Icon unclear, needs tooltip or redesign
```

**Example - Status effects:**

```
Show "burning" icon (flame symbol)
10/10 testers: "Fire damage" or "burning"
Clear icon.

Show "enfeebled" icon (abstract purple swirl)
2/10 testers correct, 8/10 "I don't know"
Unclear icon.
Action: Add tooltip OR use more intuitive icon (muscle with X?) OR remove status complexity.
```

### 6.4 The Color Blindness Test

**Method:**

1. Test with color blind testers OR use color blindness simulator
2. Check if critical information is color-only

**Analysis:**

```
Information distinguishable without color → Accessible
Information requires color → Will confuse color blind players
Provide colorblind modes if needed
```

**Common issue:** Red/green for health/poison (10% of males can't distinguish)

***

## Part 7: Load Across Loop Scales Validation

**Goal:** Verify load is appropriate at each temporal scale (micro/meso/macro/meta).

### 7.1 Micro Loop Load Test (Seconds to Minutes)

**Focus:** Execution load, instant feedback

**Test 28: The Flow State Entry Time**

**Method:**

* Time how long until player reaches flow state in micro loop
* Flow indicators: Time distortion, focused attention, smooth action

**Analysis:**

```
<2 minutes → Excellent (micro loop immediately engaging)
2-5 minutes → Good (some warm-up needed)
5-10 minutes → Slow (may be too complex for micro)
>10 minutes or never → Micro loop not engaging
```

**Common problem:** Too much decision/tracking load at micro scale
**Solution:** Automate more, simplify choices, instant feedback

### 7.2 Meso Loop Load Test (Minutes to Half Hour)

**Focus:** Decision load, goal clarity

**Test 29: The Goal Articulation Test**

**Method:**

* During meso loop, ask "What are you trying to accomplish right now?"
* Clear immediate answer → Goal clarity good

**Analysis:**

```
Immediate clear answer → Meso goal clear
Hesitation or vague answer → Goal unclear
"I don't know" → Critical goal clarity failure
```

**Common problem:** Too many simultaneous meso goals
**Solution:** Focus on one primary objective, secondary objectives optional

### 7.3 Macro Loop Load Test (Session to Run)

**Focus:** Tracking load, state persistence

**Test 30: The Session Boundary Test**

**Method:**

* Players stop mid-macro loop, return 24 hours later
* Observe re-orientation time
* Ask "Where were you in campaign/progression?"

**Analysis:**

```
<2 minutes to re-orient → Good state persistence
2-5 minutes → Acceptable with journal/recap
5-10 minutes → Poor resumption support
>10 minutes → Critical problem
```

**Common problem:** Too much state to reconstruct after session gap
**Solution:** Session recaps, journal systems, clear progress markers

### 7.4 Meta Loop Load Test (Between Sessions, Long-Term)

**Focus:** Memory load, persistent motivation

**Test 31: The Return Motivation Test**

**Method:**

* Interview after 1 week gap: "Why did you come back?"
* "What are you working toward long-term?"

**Analysis:**

```
"I wanted to see what happens next" → Intrinsic motivation (good)
"I wanted to unlock X" → Goal-driven motivation (good)
"I should finish this" → Obligation (bad, not sustainable)
"I don't remember what I was doing" → Memory load too high
```

**Common problem:** Meta goals too complex or too numerous
**Solution:** Simplify meta progression, make goals visible and memorable

***

## Part 8: Iteration and Change Tracking

### 8.1 Cognitive Load Change Log Template

```markdown
# Cognitive Load Change Log

## Change #[Number]: [System/Feature] - [Date]

### Problem Identified
**Load Type Affected:** [Tracking/Decision/Execution/Total]
**Symptoms:** [What players experienced]
**Playtest Evidence:** [Which sessions, which testers]
**Specific Behaviors:**
- [Observable behavior 1]
- [Observable behavior 2]

### Root Cause Analysis
**Diagnosis:** [Why is load too high/low]
**Investment-Payoff Assessment:** [Does this serve aesthetic?]

### Change Made
**Before:**
- [System configuration/parameters/UI]
**After:**
- [New configuration]
**Strategy Used:** [Automation/Chunking/Progressive Disclosure/Offloading/Simplification]

### Hypothesis
We expect: [Predicted outcome]
Success metrics: [How we'll measure improvement]

### Validation Results
**Playtest Sessions:** [Number, tester profiles]
**Quantitative Data:**
- [Metric before] → [Metric after]
**Qualitative Feedback:**
- [Player statements]
**Behavioral Changes:**
- [Observed differences]

### Outcome
- [ ] Success - Load now appropriate
- [ ] Partial - Improved but needs more iteration
- [ ] Failure - Reverted or trying different approach
- [ ] Trade-off - Fixed load but affected other area

### Next Steps
[What to do next]
```

### 8.2 A/B Testing Cognitive Load Changes

**When to use:** Testing load reduction strategies with similar players

**Method:**

1. Group A plays version with change
2. Group B plays version without change
3. Compare session duration, engagement, comprehension

**Metrics to compare:**

* Tutorial completion rate
* Session duration
* Return rate
* Flow state indicators
* Player satisfaction ratings
* Task completion time
* Error rate

**Example:**

```
Testing: Reduce resource count from 6 to 4 (chunking strategy)

Group A (4 resources): 
- Tutorial: 85% completion, 8 min average
- Session: 45 min average, high engagement
- Return rate: 70%

Group B (6 resources):
- Tutorial: 60% completion, 12 min average
- Session: 25 min average, signs of overwhelm
- Return rate: 45%

Result: 4 resources significantly better across all metrics.
Action: Adopt 4-resource design.
```

***

## Part 9: Special Considerations

### 9.1 Testing with Target Audience

**Critical: Test with ACTUAL target audience, not just available testers**

**Casual audience testing:**

* Recruit non-gamers or very light gamers
* Test in short sessions (15-30 min max)
* Observe if they can pick up and play immediately
* Expect low tolerance for complexity

**Midcore audience testing:**

* Recruit regular gamers
* Test in 30-90 min sessions
* Observe balance of depth and accessibility
* Can handle some complexity but not overwhelming

**Hardcore audience testing:**

* Recruit dedicated gamers
* Test in long sessions (90+ min)
* Observe if they find sufficient depth
* May complain if TOO simple

**Mixed audience testing:**

* Useful for games with scalable complexity
* Look for clear audience splits in feedback
* Don't try to please everyone - focus on primary audience

### 9.2 Testing with Different Expertise Levels

**Novice testing (First playthrough):**

* Tutorial comprehension
* Initial overwhelm
* Learning curve
* Time to "get it"

**Intermediate testing (5-20 hours):**

* System mastery development
* Strategic depth emergence
* Sustained engagement
* Skill progression

**Expert testing (50+ hours):**

* Endgame complexity
* Optimization behavior
* Depth sufficiency
* Long-term retention

**Each expertise level tests different aspects of load curve:**

* Novices test if initial load appropriate
* Intermediates test if progression satisfying
* Experts test if endgame depth sufficient

### 9.3 Testing LLM-Integrated Cognitive Load Support

**\[LLM-INTEGRATION-TESTING]**

**Special considerations when LLMs reduce cognitive load:**

**Test 32: The Automation Quality Test**

**Method:**

* LLM handles tracking/summarization
* Compare to manual tracking (human does same task)
* Measure: Accuracy, speed, player satisfaction

**Validation questions:**

* Does LLM reduce load without reducing engagement?
* Do players trust LLM summaries?
* Does LLM make errors that break immersion?
* Does LLM create new problems (over-reliance, learned helplessness)?

**Example applications:**

* LLM-generated session recaps (reduces memory load)
* LLM-managed quest journal (reduces tracking load)
* LLM-suggested strategies (reduces decision load - careful, may reduce engagement)

**Testing protocol:**

1. With LLM: Measure session duration, re-orientation time, player satisfaction
2. Without LLM: Same measurements
3. Compare: Does LLM help more than it hurts?

**Red flags:**

* Players become passive (LLM doing thinking for them)
* Trust issues (LLM makes mistakes, breaks immersion)
* Dependency (can't play without LLM assistance)

**Success indicators:**

* Faster re-orientation without loss of engagement
* Confidence in decisions with LLM support
* Optional usage (can play with or without)

***

## Part 10: Quick Reference - Testing Priority by Stage

### Pre-Production (Concept/Prototype)

**Priority: Validate core loop load is appropriate**

* Tests: 14 (Session Duration), 15 (One More), 17 (Tutorial Completion)
* Goal: Confirm core experience achievable for target audience

### Early Production (Vertical Slice)

**Priority: Validate load across all types**

* Tests: 1-5 (Tracking), 6-9 (Decision), 10-13 (Execution)
* Goal: Identify load problems early, before building full content

### Mid Production (Content Complete)

**Priority: Validate load across full experience**

* Tests: 14-17 (Total Load), 28-31 (Loop Scales)
* Goal: Ensure load sustainable across campaign/full game

### Late Production (Polish)

**Priority: Optimize UI and information architecture**

* Tests: Interface tests (6.1-6.4)
* Goal: Minimize unnecessary load through better presentation

### Post-Launch (Live Ops)

**Priority: Monitor player retention and load problems**

* Metrics: Session duration, return rate, tutorial completion
* Goal: Identify if updates adding too much complexity

***

## Part 11: Documentation Templates

### Playtest Session Report (Cognitive Load Focus)

```markdown
# Cognitive Load Playtest Session [Number]

## Session Info
- Date: [Date]
- Playtester: [Name/ID, experience level]
- Target Audience: [Casual/Midcore/Hardcore]
- Build Version: [Version]
- Duration: [Minutes]
- Session Type: [First time/Return player]

## Load Budget Assessment

Target Budget: [3-5/5-8/8-15+ units]
Estimated Current Load: [Units - based on observations]

By Source:
- Core Mechanic: [Observed load]
- Resources: [Observed load]
- Goals: [Observed load]
- Context/Meta: [Observed load]

## Tracking Load Observations

UI Check Frequency:
- [Element]: [Checks per minute]
- [Element]: [Checks per minute]

Interruption Recovery:
- Interrupted at [timestamp]
- Recovery time: [seconds]
- Could resume confidently: [Yes/No]

Memory Test:
- Asked to state [information] without looking
- Accuracy: [X/Y correct]

External Notes:
- [What player wrote down]
- [Why they felt they needed to externalize]

## Decision Load Observations

Decision Times:
| Decision Type | Time Taken | Confident? | Outcome |
|--------------|------------|------------|---------|
| [Action]     | [Seconds]  | [Y/N]      | [Result]|

Analysis Paralysis Incidents:
- [Timestamp] [Decision] took [time] - [player behavior]

Strategy Diversity:
- [What approaches did player try]
- [Did they explore options or default to one strategy]

## Execution Load Observations

Success Rates:
- [Action]: [X/Y successful]
- Improvement visible: [Yes/No]

Execution Errors:
- [Timestamp] Failed [action] - [Knowledge gap/Skill gap/UI issue]

Stress Response:
- Performance under pressure: [Better/Same/Worse than practice]

## Total Load Indicators

Session Duration: [Minutes]
Stop Reason: [Satisfied/Fatigued/Bored/External]
"One More" Observed: [Yes/No]
Flow State Achieved: [Yes/No, when]

Fatigue Indicators:
- Decision quality degradation: [Yes/No, when]
- Execution performance degradation: [Yes/No, when]
- Engagement level drop: [Yes/No, when]

## Tutorial/Onboarding (if applicable)

Completion: [Yes/No]
Time: [Minutes]
Dropout point: [Where, if applicable]
Comprehension: [Player could explain mechanics afterward: Y/N]

## Verbal Feedback

Unprompted statements:
- [Timestamp] "[Direct quote]"

Post-session interview:
Q: [Question]
A: [Response]

## Load Problem Indicators

**Red Flags:**
- [ ] Constant UI checking (>6 per minute)
- [ ] Analysis paralysis (>5 min decisions)
- [ ] Can't remember basic information
- [ ] Early session termination due to fatigue
- [ ] Can't resume after interruption
- [ ] "It's too complicated" statements
- [ ] External tools needed for basic play

**Positive Signs:**
- [ ] Flow state achieved
- [ ] "One more" continuation
- [ ] Confident decision-making
- [ ] Internalizes important information
- [ ] Returns for multiple sessions
- [ ] Improvement across sessions

## Investment-Payoff Analysis

For each major system/feature:
| System | Load Created | Aesthetic Payoff | Verdict |
|--------|--------------|------------------|---------|
| [Name] | [H/M/L]      | [H/M/L]          | [Keep/Reduce/Cut] |

## Recommendations

Priority | Change | Rationale | Expected Impact
---------|--------|-----------|----------------
High     | [Change] | [Why] | [What will improve]
Medium   | [Change] | [Why] | [What will improve]
Low      | [Change] | [Why] | [What will improve]

## Next Steps
[What to test next / What changes to make / What to validate]
```

***

## Conclusion

Cognitive load validation is **observational and iterative**.

**Core principles:**

1. **Define load budget before testing** - Know your constraints
2. **Observe behaviors, not just feedback** - What players do > what they say
3. **Test with actual target audience** - Casual/midcore/hardcore have different capacities
4. **Measure across all load types** - Tracking, Decision, Execution, Total
5. **Validate investment-payoff** - Every feature must justify its cognitive cost

**Integration points:**

* **Chapter 1:** Target audience determines load budget
* **Chapter 2:** Archetype determines natural load profile
* **Chapter 3:** Resources create tracking load
* **Chapter 4:** This guide validates the design decisions made using Chapter 4 framework
* **Chapter 5:** Balance must be perceivable within cognitive budget
* **Chapter 6:** Load distributes across loop scales, test each scale appropriately

**Remember:** Cognitive load isn't good or bad - it's investment that must match aesthetic goals and audience capacity. High load serves Mastery, low load serves Submission. Your job: validate you've matched load to your aesthetic and audience.

**When in doubt:** Run the Rapid Diagnostic Checklist (Section 4.2). It catches 80% of load problems quickly.

**The ultimate test:** Do players achieve flow state? Can they play for target duration without fatigue? Do they return for more sessions? If yes to all three, your cognitive load is working.

Good luck, and observe carefully.
